import streamlit as st
from langchain.chat_models import ChatOpenAI
from langchain.schema import SystemMessage, HumanMessage
from google.oauth2 import service_account
from googleapiclient.discovery import build
import json
import re
import os
import requests

# --- ãƒšãƒ¼ã‚¸è¨­å®š ---
st.set_page_config(page_title="RAGã‚¹ã‚«ã‚¦ãƒˆæ–‡ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ v3.5")
st.title("ğŸ§  RAG Ã— ã‚¹ã‚«ã‚¦ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬è‡ªå‹•ç”Ÿæˆ v3.5")

# --- APIã‚­ãƒ¼ ---
openai_api_key = os.environ.get("OPENAI_API_KEY") or st.secrets.get("OPENAI_API_KEY")
serpapi_key = os.environ.get("SERPAPI_KEY") or st.secrets.get("SERPAPI_KEY")

# --- å…¥åŠ›æ¬„ ---
candidate_profile = st.text_area("ğŸ“„ å€™è£œè€…ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ã‚’è²¼ã£ã¦ãã ã•ã„")
fishing_job_1 = st.text_input("ğŸ¯ é‡£ã‚Šæ±‚äººâ‘ ï¼ˆç¤¾å_æ±‚äººã‚¿ã‚¤ãƒˆãƒ«ï¼‰")
fishing_job_2 = st.text_input("ğŸ¯ é‡£ã‚Šæ±‚äººâ‘¡ï¼ˆç¤¾å_æ±‚äººã‚¿ã‚¤ãƒˆãƒ«ï¼‰")
fishing_job_3 = st.text_input("ğŸ¯ é‡£ã‚Šæ±‚äººâ‘¢ï¼ˆç¤¾å_æ±‚äººã‚¿ã‚¤ãƒˆãƒ«ï¼‰")
contact_person = st.text_input("ğŸ§‘â€ğŸ’¼ ã‚¹ã‚«ã‚¦ãƒˆé€ä¿¡è€…åï¼ˆç½²åã«è¡¨ç¤ºï¼‰")
generate_button = st.button("ğŸš€ ã‚¹ã‚«ã‚¦ãƒˆæ–‡ã‚’ç”Ÿæˆ")

# --- Google Driveã‹ã‚‰ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå–å¾—é–¢æ•° ---
def find_doc_content_by_keyword(keyword: str):
    SCOPES = ['https://www.googleapis.com/auth/drive.readonly']
    raw_json = st.secrets["SERVICE_ACCOUNT_JSON"]
    creds_dict = json.loads(raw_json)
    creds = service_account.Credentials.from_service_account_info(creds_dict, scopes=SCOPES)
    drive_service = build('drive', 'v3', credentials=creds)
    query = f"mimeType='application/vnd.google-apps.document' and name contains '{keyword.split('_')[0]}'"
    results = drive_service.files().list(q=query, fields="files(id, name)", pageSize=5).execute()
    files = results.get('files', [])

    if not files:
        return ""

    file_id = files[0]['id']
    doc_service = build('docs', 'v1', credentials=creds)
    doc = doc_service.documents().get(documentId=file_id).execute()

    content = ""
    for element in doc.get("body").get("content"):
        if "paragraph" in element:
            for elem in element["paragraph"].get("elements", []):
                text_run = elem.get("textRun")
                if text_run:
                    content += text_run.get("content")

    cleaned = re.sub(r'\n{2,}', '\n\n', content.strip())
    return cleaned

# --- SerpAPIã‹ã‚‰æ¤œç´¢çµæœå–å¾— ---
def get_serp_snippets(query):
    url = "https://serpapi.com/search.json"
    params = {
        "q": query,
        "hl": "ja",
        "gl": "jp",
        "api_key": serpapi_key
    }
    response = requests.get(url, params=params)
    data = response.json()
    snippets = []
    for res in data.get("organic_results", [])[:2]:
        title = res.get("title", "")
        snippet = res.get("snippet", "")
        snippets.append(f"ã€{title}ã€‘\n{snippet}")
    return "\n\n".join(snippets)

# --- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ ---
def build_prompt(profile, rag_summary, serp_summary, jobs, sender):
    jobs_bullet = "\n".join([f"â˜…{j}" for j in jobs if j])

    return f"""
ã‚ãªãŸã¯è¶…ä¸€æµã®ã‚¹ã‚«ã‚¦ãƒˆãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚
ä»¥ä¸‹ã®æƒ…å ±ã¨æ§‹é€ ã‚’å³å®ˆã—ã€å€™è£œè€…ãŒã€Œå¿œå‹Ÿã—ãŸããªã‚‹ã€ã€Œè©±ã‚’èããŸããªã‚‹ã€ä»¶åï¼‹æœ¬æ–‡ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

ã€ã‚¹ã‚«ã‚¦ãƒˆæ–‡æ§‹é€ ï¼ˆå®Œå…¨å›ºå®šï¼‰ã€‘
1. ä»¶åï¼šå…·ä½“æ€§ãƒ»é™å®šæ€§ãƒ»æˆ¦ç•¥æ€§ãƒ»å¹´åãƒ»æ¥­ç•Œåã‚’å«ã‚€50æ–‡å­—ä»¥å†…
2. å†’é ­ã‚­ãƒ£ãƒƒãƒï¼š"ã‚ãªãŸã®æ¬¡ã®ã‚­ãƒ£ãƒªã‚¢ã‚¹ãƒ†ãƒƒãƒ—ã‚’ã€ç§ãŸã¡ã¨å…±ã«ã€‚"
3. å…±æ„Ÿå°å…¥ï¼šå€™è£œè€…ã®â—¯â—¯çµŒé¨“ã«å…±æ„Ÿãƒ»æœŸå¾…ï¼ˆ2æ–‡ï¼‰
4. SIESTAã®æ”¯æ´å®Ÿç¸¾ï¼šä¸Šä½1%ã€å¹´åUPã€å…·ä½“äº‹ä¾‹ï¼ˆçŸ­ãã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆã‚ã‚‹è¡¨ç¾ï¼‰
5. é­…åŠ›è¨´æ±‚ãƒ–ãƒ­ãƒƒã‚¯ï¼ˆ3ç¤¾åˆ†ï¼‰:
  - ä¼šç¤¾å
  - ãƒã‚¸ã‚·ãƒ§ãƒ³å
  - Webæ¤œç´¢ã‹ã‚‰å¾—ãŸç‰¹å¾´ï¼ˆå¼·ã¿ã€æˆé•·æ€§ã€æ¡ä»¶ï¼‰
  - ã€Œã‚ãªãŸã®â—¯â—¯çµŒé¨“ãŒæ´»ã‹ã•ã‚Œã‚‹ã€ä¸€è¨€ï¼ˆãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã«åˆã‚ã›ã¦ï¼‰
6. ç· ã‚ï¼šã‚«ã‚¸ãƒ¥ã‚¢ãƒ«é¢è«‡ææ¡ˆã€å®‰å¿ƒæ„Ÿã®ã‚ã‚‹ãƒˆãƒ¼ãƒ³
7. ç½²åï¼šæ–‡æœ«ã«ã®ã¿è¡¨ç¤ºï¼ˆ{sender}ï½œSIESTAä»£è¡¨ï¼‰

ã€å€™è£œè€…ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ã€‘
{profile}

ã€é‡£ã‚Šæ±‚äººï¼ˆå…¥åŠ›å†…å®¹ï¼‰ã€‘
{jobs_bullet}

ã€é‡£ã‚Šæ±‚äººã®æ¤œç´¢çµæœï¼ˆSerpAPIï¼‰ã€‘
{serp_summary}

ã€Driveã‹ã‚‰å–å¾—ã—ãŸä¼æ¥­ãƒŠãƒ¬ãƒƒã‚¸ï¼ˆRAGï¼‰ã€‘
{rag_summary}

ã€å‡ºåŠ›å½¢å¼ã€‘
ä»¶åï¼šâ—¯â—¯â—¯â—¯ï¼ˆ50æ–‡å­—ä»¥å†…ï¼‰
æœ¬æ–‡ï¼š

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ã‚ãªãŸã®æ¬¡ã®ã‚­ãƒ£ãƒªã‚¢ã‚¹ãƒ†ãƒƒãƒ—ã‚’ã€ç§ãŸã¡ã¨å…±ã«ã€‚
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

â—¯â—¯â—¯â—¯ï¼ˆ1800æ–‡å­—å‰å¾Œï¼‰

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
{sender}ï½œSIESTAä»£è¡¨
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""

# --- å®Ÿè¡Œãƒ–ãƒ­ãƒƒã‚¯ ---
if generate_button and openai_api_key and candidate_profile:
    rag_summary = ""
    serp_summary = ""
    jobs = [fishing_job_1, fishing_job_2, fishing_job_3]

    for keyword in jobs:
        if keyword:
            st.info(f"ğŸ” {keyword} ã®ä¼æ¥­ãƒŠãƒ¬ãƒƒã‚¸ã‚’å–å¾—ä¸­...")
            content = find_doc_content_by_keyword(keyword)
            if content:
                rag_summary += f"ã€{keyword}ã€‘\n{content}\n\n"
            st.info(f"ğŸŒ {keyword} ã‚’Webæ¤œç´¢ä¸­...")
            serp = get_serp_snippets(keyword)
            if serp:
                serp_summary += f"ã€{keyword}ã€‘\n{serp}\n\n"

    if serp_summary:
        with st.expander("ğŸŒ Webæ¤œç´¢çµæœï¼ˆSerpAPIï¼‰"):
            st.markdown(serp_summary)

    if rag_summary:
        with st.expander("ğŸ“‚ DriveãƒŠãƒ¬ãƒƒã‚¸ï¼ˆRAGï¼‰"):
            st.markdown(rag_summary)

    st.info("ğŸ¤– GPTã§æ–‡é¢ç”Ÿæˆä¸­...")
    llm = ChatOpenAI(model_name="gpt-4o", temperature=0.7, openai_api_key=openai_api_key, max_tokens=1800)
    prompt = build_prompt(candidate_profile, rag_summary, serp_summary, jobs, contact_person)
    messages = [
        SystemMessage(content="ã‚ãªãŸã¯ãƒ—ãƒ­ã®ã‚¹ã‚«ã‚¦ãƒˆæ–‡ä½œæˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã™ã€‚æ§‹æˆã€ãƒˆãƒ¼ãƒ³ã€è¨´æ±‚è»¸ã‚’å³å¯†ã«å®ˆã£ã¦ãã ã•ã„ã€‚"),
        HumanMessage(content=prompt)
    ]
    response = llm(messages)

    st.success("âœ… ã‚¹ã‚«ã‚¦ãƒˆæ–‡ãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸ")
    st.markdown("""---""")
    st.markdown(response.content)

elif generate_button and not openai_api_key:
    st.error("OpenAI API Key ã‚’ç’°å¢ƒå¤‰æ•°ã¾ãŸã¯Secretsã«è¨­å®šã—ã¦ãã ã•ã„")
